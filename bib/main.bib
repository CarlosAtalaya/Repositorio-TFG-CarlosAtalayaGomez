% Referencias bibliográficas para TFG
% Vision Transformers para Detección de Anomalías Industriales
% Generado: Octubre 2024

% ============================================================================
% SURVEYS Y FUNDAMENTOS
% ============================================================================

@article{khan2024survey,
  title     = {A Survey of the Self Supervised Learning Mechanisms for Vision Transformers},
  author    = {Khan, Asifullah and Sohail, Anabia and Fiaz, Mustansar and Hassan, Mehdi and Afridi, Tariq Habib and Marwat, Sibghat Ullah and Munir, Farzeen and Ali, Safdar and Naseem, Hannan and Zaheer, Muhammad Zaigham and Ali, Kamran and Sultana, Tangina and Tanoli, Ziaurrehman and Akhter, Naeem},
  journal   = {arXiv preprint},
  year      = {2024},
  note      = {arXiv:2408.17059 [cs]},
  doi       = {10.48550/arXiv.2408.17059},
  url       = {http://arxiv.org/abs/2408.17059},
  keywords  = {self-supervised learning, vision transformers, SSL}
}

@inproceedings{sudarshan2025object,
  title     = {Object Detection using Vision Transformer and Deep Learning for Computer Vision Applications},
  author    = {Sudarshan, S. and Mohana and Ambika, G. and Kavitha, A. and Nataraj, K. and Sudhangowda, B. S.},
  booktitle = {2025 7th International Conference on Intelligent Sustainable Systems (ICISS)},
  pages     = {1524--1529},
  year      = {2025},
  month     = {March},
  doi       = {10.1109/ICISS63372.2025.11076364},
  url       = {https://ieeexplore.ieee.org/document/11076364},
  keywords  = {vision transformer, object detection, CNN comparison}
}

% ============================================================================
% VIT PARA ANOMALY DETECTION INDUSTRIAL
% ============================================================================

@article{zhang2025exploring,
  title     = {Exploring plain ViT features for multi-class unsupervised visual anomaly detection},
  author    = {Zhang, Jiangning and Chen, Xuhai and Wang, Yabiao and Wang, Chengjie and Liu, Yong and Li, Xiangtai and Yang, Ming-Hsuan and Tao, Dacheng},
  journal   = {Computer Vision and Image Understanding},
  volume    = {253},
  pages     = {104308},
  year      = {2025},
  month     = {March},
  publisher = {Elsevier},
  doi       = {10.1016/j.cviu.2025.104308},
  url       = {https://www.sciencedirect.com/science/article/pii/S1077314225000311},
  keywords  = {ViTAD, multi-class anomaly detection, plain ViT, MVTec AD}
}

@article{wang2022defect,
  title     = {Defect Transformer: An Efficient Hybrid Transformer Architecture for Surface Defect Detection},
  author    = {Wang, Junpu and Xu, Guili and Yan, Fuju and Wang, Jinjin and Wang, Zhengsheng},
  journal   = {IEEE Transactions on Instrumentation and Measurement},
  year      = {2022},
  month     = {July},
  note      = {arXiv:2207.08319 [cs]},
  doi       = {10.48550/arXiv.2207.08319},
  url       = {http://arxiv.org/abs/2207.08319},
  keywords  = {hybrid architecture, CNN-transformer, surface defect detection}
}

% ============================================================================
% TRANSFER LEARNING Y FEW-SHOT LEARNING
% ============================================================================

@inproceedings{huang2025semiconductor,
  title     = {Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers},
  author    = {Huang, Chien-Fu and Sieg, Katherine and Karlinksy, Leonid and Flores, Nash and Sheraw, Rebekah and Zhang, Xin},
  booktitle = {2025 36th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)},
  pages     = {1--5},
  year      = {2025},
  month     = {May},
  organization = {IBM Research},
  note      = {arXiv:2506.03345 [cs]},
  doi       = {10.1109/ASMC64512.2025.11010396},
  url       = {http://arxiv.org/abs/2506.03345},
  keywords  = {DINOv2, few-shot learning, semiconductor defects, transfer learning, industrial application}
}

@book{alber2024evaluating,
  title     = {Evaluating Vision Transformer Models for Visual Quality Control in Industrial Manufacturing},
  author    = {Alber, Miriam and Hönes, Christoph and Baier, Patrick},
  year      = {2024},
  series    = {Lecture Notes in Computer Science},
  volume    = {14950},
  publisher = {Springer},
  note      = {arXiv:2411.14953 [cs]},
  doi       = {10.1007/978-3-031-70381-2},
  url       = {http://arxiv.org/abs/2411.14953},
  keywords  = {benchmarking, ViT evaluation, industrial QC, MVTec AD, BTAD}
}

% ============================================================================
% TÉCNICAS AVANZADAS
% ============================================================================

@article{legia2025consistent,
  title     = {On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection},
  author    = {Le-Gia, Tai and Ahn, Jaehyun},
  journal   = {arXiv preprint},
  year      = {2025},
  month     = {October},
  note      = {arXiv:2510.10456 [cs]},
  doi       = {10.48550/arXiv.2510.10456},
  url       = {http://arxiv.org/abs/2510.10456},
  keywords  = {zero-shot, consistent anomalies, CoDeGraph, neighbor-burnout, DINOv2}
}

@article{nafez2025patchguard,
  title     = {PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies},
  author    = {Nafez, Mojtaba and Koochakian, Amirhossein and Maleki, Arad and Habibi, Jafar and Rohban, Mohammad Hossein},
  journal   = {arXiv preprint},
  year      = {2025},
  month     = {June},
  note      = {arXiv:2506.09237 [cs]},
  doi       = {10.48550/arXiv.2506.09237},
  url       = {http://arxiv.org/abs/2506.09237},
  keywords  = {adversarial robustness, pseudo-anomalies, attention mechanisms}
}

% ============================================================================
% ARQUITECTURAS FUNDACIONALES
% ============================================================================

@inproceedings{dosovitskiy2021vit,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2010.11929},
  keywords  = {ViT, vision transformer, original paper}
}

@inproceedings{he2016resnet,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {770--778},
  year      = {2016},
  doi       = {10.1109/CVPR.2016.90},
  keywords  = {ResNet, residual learning, CNN}
}

@inproceedings{tan2019efficientnet,
  title     = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author    = {Tan, Mingxing and Le, Quoc V.},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
  pages     = {6105--6114},
  year      = {2019},
  url       = {https://arxiv.org/abs/1905.11946},
  keywords  = {EfficientNet, model scaling, CNN}
}

% ============================================================================
% APRENDIZAJE AUTO-SUPERVISADO (SSL)
% ============================================================================

@article{oquab2024dinov2,
  title     = {DINOv2: Learning Robust Visual Features without Supervision},
  author    = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jégou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  journal   = {Transactions on Machine Learning Research},
  year      = {2024},
  url       = {https://arxiv.org/abs/2304.07193},
  keywords  = {DINOv2, self-supervised, vision features}
}

@inproceedings{caron2021dino,
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {9650--9660},
  year      = {2021},
  doi       = {10.1109/ICCV48922.2021.00951},
  keywords  = {DINO, self-distillation, vision transformers}
}

@inproceedings{chen2020simclr,
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  author    = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages     = {1597--1607},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.05709},
  keywords  = {SimCLR, contrastive learning, self-supervised}
}

@inproceedings{he2020moco,
  title     = {Momentum Contrast for Unsupervised Visual Representation Learning},
  author    = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {9729--9738},
  year      = {2020},
  doi       = {10.1109/CVPR42600.2020.00975},
  keywords  = {MoCo, momentum contrast, self-supervised}
}

@inproceedings{he2022mae,
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {16000--16009},
  year      = {2022},
  doi       = {10.1109/CVPR52688.2022.01553},
  keywords  = {MAE, masked autoencoder, self-supervised}
}

@inproceedings{devlin2019bert,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)},
  pages     = {4171--4186},
  year      = {2019},
  doi       = {10.18653/v1/N19-1423},
  keywords  = {BERT, language model, transformer}
}

% ============================================================================
% INTERPRETABILIDAD Y VISUALIZACIÓN
% ============================================================================

@inproceedings{selvaraju2017gradcam,
  title     = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Class Activation Mapping},
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages     = {618--626},
  year      = {2017},
  doi       = {10.1109/ICCV.2017.74},
  keywords  = {GradCAM, explainability, visualization}
}

% ============================================================================
% DATASETS Y BENCHMARKS
% ============================================================================

@inproceedings{bergmann2019mvtec,
  title     = {MVTec AD -- A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection},
  author    = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {9592--9600},
  year      = {2019},
  doi       = {10.1109/CVPR.2019.00982},
  keywords  = {MVTec AD, anomaly detection, benchmark, industrial}
}

@inproceedings{roth2022patchcore,
  title     = {Towards Total Recall in Industrial Anomaly Detection},
  author    = {Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Schölkopf, Bernhard and Schiele, Bernt and Brox, Thomas and Gehler, Peter},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {14318--14328},
  year      = {2022},
  doi       = {10.1109/CVPR52688.2022.01392},
  keywords  = {PatchCore, memory bank, anomaly detection}
}