#!/usr/bin/env python3
"""
An√°lisis de distribuci√≥n de tama√±os de im√°genes para decidir resoluci√≥n de entrenamiento
"""

import json
import os
from PIL import Image
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

# Configuraci√≥n
DATASET_BASE = Path("../../curated_dataset_splitted_20251101_provisional_1st_version")
SPLITS = ['train', 'val', 'test']

def analyze_split(split_name):
    """Analiza tama√±os de im√°genes en un split del dataset."""
    
    ann_file = DATASET_BASE / split_name / f"{split_name}.json"
    img_folder = DATASET_BASE / split_name / "images"
    
    print(f"\n{'='*80}")
    print(f"Analizando: {split_name.upper()}")
    print(f"{'='*80}")
    
    # Cargar annotations
    with open(ann_file) as f:
        data = json.load(f)
    
    sizes = []
    widths = []
    heights = []
    aspect_ratios = []
    areas = []
    
    for img_info in data['images']:
        img_path = img_folder / img_info['file_name']
        
        if not img_path.exists():
            print(f"‚ö†Ô∏è  Imagen no encontrada: {img_path}")
            continue
        
        try:
            with Image.open(img_path) as img:
                w, h = img.size
                widths.append(w)
                heights.append(h)
                sizes.append(min(w, h))  # Lado m√°s corto
                aspect_ratios.append(max(w, h) / min(w, h))
                areas.append(w * h)
        except Exception as e:
            print(f"‚ö†Ô∏è  Error leyendo {img_path}: {e}")
    
    # Convertir a arrays numpy
    widths = np.array(widths)
    heights = np.array(heights)
    sizes = np.array(sizes)
    aspect_ratios = np.array(aspect_ratios)
    areas = np.array(areas)
    
    # Estad√≠sticas
    print(f"\nüìä ESTAD√çSTICAS DE TAMA√ëOS ({len(sizes)} im√°genes)")
    print(f"{'-'*80}")
    
    print(f"\nüîπ Ancho (width):")
    print(f"   Min:     {widths.min():.0f} px")
    print(f"   Q1:      {np.percentile(widths, 25):.0f} px")
    print(f"   Median:  {np.median(widths):.0f} px")
    print(f"   Mean:    {widths.mean():.0f} px")
    print(f"   Q3:      {np.percentile(widths, 75):.0f} px")
    print(f"   Max:     {widths.max():.0f} px")
    print(f"   Std:     {widths.std():.0f} px")
    
    print(f"\nüîπ Alto (height):")
    print(f"   Min:     {heights.min():.0f} px")
    print(f"   Q1:      {np.percentile(heights, 25):.0f} px")
    print(f"   Median:  {np.median(heights):.0f} px")
    print(f"   Mean:    {heights.mean():.0f} px")
    print(f"   Q3:      {np.percentile(heights, 75):.0f} px")
    print(f"   Max:     {heights.max():.0f} px")
    print(f"   Std:     {heights.std():.0f} px")
    
    print(f"\nüîπ Lado m√°s corto (min side):")
    print(f"   Min:     {sizes.min():.0f} px")
    print(f"   Q1:      {np.percentile(sizes, 25):.0f} px")
    print(f"   Median:  {np.median(sizes):.0f} px")
    print(f"   Mean:    {sizes.mean():.0f} px")
    print(f"   Q3:      {np.percentile(sizes, 75):.0f} px")
    print(f"   Max:     {sizes.max():.0f} px")
    
    print(f"\nüîπ Aspect Ratio (max/min):")
    print(f"   Min:     {aspect_ratios.min():.2f}")
    print(f"   Median:  {np.median(aspect_ratios):.2f}")
    print(f"   Mean:    {aspect_ratios.mean():.2f}")
    print(f"   Max:     {aspect_ratios.max():.2f}")
    
    print(f"\nüîπ √Årea (p√≠xeles):")
    print(f"   Min:     {areas.min():.0f} px¬≤ ({areas.min()/1e6:.2f} MP)")
    print(f"   Median:  {np.median(areas):.0f} px¬≤ ({np.median(areas)/1e6:.2f} MP)")
    print(f"   Mean:    {areas.mean():.0f} px¬≤ ({areas.mean()/1e6:.2f} MP)")
    print(f"   Max:     {areas.max():.0f} px¬≤ ({areas.max()/1e6:.2f} MP)")
    
    # Distribuci√≥n por rangos
    print(f"\nüì¶ DISTRIBUCI√ìN POR RANGOS (lado m√°s corto)")
    print(f"{'-'*80}")
    
    ranges = [
        (0, 500, "Muy peque√±as (<500px)"),
        (500, 800, "Peque√±as (500-800px)"),
        (800, 1200, "Medianas (800-1200px)"),
        (1200, 1600, "Grandes (1200-1600px)"),
        (1600, 2000, "Muy grandes (1600-2000px)"),
        (2000, 10000, "Extra grandes (>2000px)")
    ]
    
    for min_val, max_val, label in ranges:
        count = np.sum((sizes >= min_val) & (sizes < max_val))
        pct = 100 * count / len(sizes)
        bar = '‚ñà' * int(pct / 2)
        print(f"   {label:30s} {count:4d} ({pct:5.1f}%) {bar}")
    
    # Identificar extremos
    print(f"\nüîç IM√ÅGENES EXTREMAS")
    print(f"{'-'*80}")
    
    # 5 m√°s peque√±as
    smallest_idx = np.argsort(sizes)[:5]
    print(f"\nüîª 5 m√°s peque√±as:")
    for idx in smallest_idx:
        img_info = data['images'][idx]
        print(f"   {img_info['file_name']:50s} {widths[idx]:.0f}√ó{heights[idx]:.0f} px")
    
    # 5 m√°s grandes
    largest_idx = np.argsort(sizes)[-5:][::-1]
    print(f"\nüî∫ 5 m√°s grandes:")
    for idx in largest_idx:
        img_info = data['images'][idx]
        print(f"   {img_info['file_name']:50s} {widths[idx]:.0f}√ó{heights[idx]:.0f} px")
    
    return {
        'split': split_name,
        'n_images': len(sizes),
        'widths': widths,
        'heights': heights,
        'sizes': sizes,
        'aspect_ratios': aspect_ratios,
        'areas': areas
    }


def recommend_resolution(all_stats):
    """Recomienda resoluci√≥n de entrenamiento basada en an√°lisis."""
    
    # Combinar todos los splits
    all_sizes = np.concatenate([s['sizes'] for s in all_stats])
    all_widths = np.concatenate([s['widths'] for s in all_stats])
    all_heights = np.concatenate([s['heights'] for s in all_stats])
    
    print(f"\n{'='*80}")
    print(f"üí° RECOMENDACIONES DE RESOLUCI√ìN")
    print(f"{'='*80}")
    
    # Percentiles clave
    p10 = np.percentile(all_sizes, 10)
    p25 = np.percentile(all_sizes, 25)
    p50 = np.percentile(all_sizes, 50)
    p75 = np.percentile(all_sizes, 75)
    p90 = np.percentile(all_sizes, 90)
    
    print(f"\nüìê Percentiles del lado m√°s corto:")
    print(f"   P10: {p10:.0f} px")
    print(f"   P25: {p25:.0f} px")
    print(f"   P50 (mediana): {p50:.0f} px")
    print(f"   P75: {p75:.0f} px")
    print(f"   P90: {p90:.0f} px")
    
    # Encontrar m√∫ltiplos de 14 m√°s cercanos
    def nearest_multiple_14(val):
        return int(round(val / 14) * 14)
    
    # Opciones de resoluci√≥n
    print(f"\nüéØ OPCIONES DE RESOLUCI√ìN (m√∫ltiplos de 14 para ViT):")
    print(f"{'-'*80}")
    
    options = [
        (p25, "Conservadora", "Minimiza upscaling de im√°genes peque√±as"),
        (p50, "Balanceada", "Compromiso entre peque√±as y grandes"),
        (p75, "Agresiva", "Preserva m√°s detalles de im√°genes grandes"),
        (1120, "Est√°ndar ViT", "Tama√±o com√∫n en papers (1120√ó1120)"),
        (1400, "Alta resoluci√≥n", "M√°xima calidad (requiere m√°s memoria)"),
    ]
    
    print(f"\n{'Opci√≥n':<20} {'Resoluci√≥n':<15} {'Justificaci√≥n':<40} {'Impacto':<30}")
    print(f"{'-'*110}")
    
    for base_val, name, reason in options:
        res = nearest_multiple_14(base_val)
        
        # Calcular impacto
        upscale = np.sum(all_sizes < res) / len(all_sizes) * 100
        downscale = np.sum(all_sizes > res) / len(all_sizes) * 100
        
        print(f"{name:<20} {res}√ó{res:<10} {reason:<40} ‚Üë{upscale:.0f}% ‚Üì{downscale:.0f}%")
    
    # Recomendaci√≥n final
    print(f"\n‚úÖ RECOMENDACI√ìN FINAL:")
    print(f"{'-'*80}")
    
    recommended = nearest_multiple_14(p50)
    
    print(f"""
Para tu dataset con:
- Rango: {all_sizes.min():.0f}px - {all_sizes.max():.0f}px
- Mediana: {p50:.0f}px

Recomiendo: {recommended}√ó{recommended} px

RAZONES:
1. ‚úì M√∫ltiplo de 14 (compatible con DINOv3 patch size)
2. ‚úì Cerca de la mediana del dataset
3. ‚úì Balance entre upscaling ({np.sum(all_sizes < recommended) / len(all_sizes) * 100:.0f}% im√°genes) 
   y downscaling ({np.sum(all_sizes > recommended) / len(all_sizes) * 100:.0f}% im√°genes)
4. ‚úì Manejable en RTX 4070 12GB con batch_size=1

ALTERNATIVAS:
- Si OOM ‚Üí {nearest_multiple_14(p25)}√ó{nearest_multiple_14(p25)} (m√°s conservador)
- Si quieres m√°xima calidad ‚Üí 1400√ó1400 (requiere m√°s memoria)

CONFIG YAML:
  - {{type: Resize, size: [{recommended}, {recommended}]}}
  collate_fn:
    base_size: {recommended}
  eval_spatial_size: [{recommended}, {recommended}]
""")
    
    return recommended


def plot_distributions(all_stats, output_dir='analysis_plots'):
    """Genera gr√°ficas de distribuci√≥n."""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Combinar todos los splits
    all_widths = np.concatenate([s['widths'] for s in all_stats])
    all_heights = np.concatenate([s['heights'] for s in all_stats])
    all_sizes = np.concatenate([s['sizes'] for s in all_stats])
    all_aspect_ratios = np.concatenate([s['aspect_ratios'] for s in all_stats])
    
    # Plot 1: Histograma de tama√±os
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    axes[0, 0].hist(all_widths, bins=50, alpha=0.7, color='blue', edgecolor='black')
    axes[0, 0].axvline(np.median(all_widths), color='red', linestyle='--', label=f'Mediana: {np.median(all_widths):.0f}')
    axes[0, 0].set_xlabel('Ancho (p√≠xeles)', fontsize=12)
    axes[0, 0].set_ylabel('Frecuencia', fontsize=12)
    axes[0, 0].set_title('Distribuci√≥n de Anchos', fontsize=14, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)
    
    axes[0, 1].hist(all_heights, bins=50, alpha=0.7, color='green', edgecolor='black')
    axes[0, 1].axvline(np.median(all_heights), color='red', linestyle='--', label=f'Mediana: {np.median(all_heights):.0f}')
    axes[0, 1].set_xlabel('Alto (p√≠xeles)', fontsize=12)
    axes[0, 1].set_ylabel('Frecuencia', fontsize=12)
    axes[0, 1].set_title('Distribuci√≥n de Altos', fontsize=14, fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)
    
    axes[1, 0].hist(all_sizes, bins=50, alpha=0.7, color='purple', edgecolor='black')
    axes[1, 0].axvline(np.median(all_sizes), color='red', linestyle='--', label=f'Mediana: {np.median(all_sizes):.0f}')
    axes[1, 0].set_xlabel('Lado m√°s corto (p√≠xeles)', fontsize=12)
    axes[1, 0].set_ylabel('Frecuencia', fontsize=12)
    axes[1, 0].set_title('Distribuci√≥n de Lado M√°s Corto', fontsize=14, fontweight='bold')
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)
    
    axes[1, 1].hist(all_aspect_ratios, bins=50, alpha=0.7, color='orange', edgecolor='black')
    axes[1, 1].axvline(np.median(all_aspect_ratios), color='red', linestyle='--', label=f'Mediana: {np.median(all_aspect_ratios):.2f}')
    axes[1, 1].set_xlabel('Aspect Ratio', fontsize=12)
    axes[1, 1].set_ylabel('Frecuencia', fontsize=12)
    axes[1, 1].set_title('Distribuci√≥n de Aspect Ratio', fontsize=14, fontweight='bold')
    axes[1, 1].legend()
    axes[1, 1].grid(alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'image_size_distributions.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\n‚úì Gr√°fica guardada: {output_path}")
    plt.close()
    
    # Plot 2: Scatter ancho vs alto
    fig, ax = plt.subplots(figsize=(12, 10))
    
    for stats in all_stats:
        ax.scatter(stats['widths'], stats['heights'], 
                  alpha=0.6, s=30, label=stats['split'].capitalize())
    
    ax.set_xlabel('Ancho (p√≠xeles)', fontsize=12)
    ax.set_ylabel('Alto (p√≠xeles)', fontsize=12)
    ax.set_title('Distribuci√≥n Ancho vs Alto por Split', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # A√±adir l√≠neas de referencia para resoluciones comunes
    for res in [640, 896, 1120, 1400]:
        ax.axhline(res, color='gray', linestyle=':', alpha=0.5, linewidth=1)
        ax.axvline(res, color='gray', linestyle=':', alpha=0.5, linewidth=1)
        ax.text(res + 50, ax.get_ylim()[1] * 0.95, f'{res}px', 
               fontsize=9, color='gray', alpha=0.7)
    
    plt.tight_layout()
    output_path = os.path.join(output_dir, 'width_vs_height_scatter.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"‚úì Gr√°fica guardada: {output_path}")
    plt.close()


def main():
    """Ejecuta an√°lisis completo."""
    
    print("="*80)
    print("üî¨ AN√ÅLISIS DE DISTRIBUCI√ìN DE TAMA√ëOS - DATASET INDUSTRIAL")
    print("="*80)
    
    # Verificar que existe el dataset
    if not DATASET_BASE.exists():
        print(f"\n‚ùå Error: No se encuentra el dataset en {DATASET_BASE}")
        print("   Por favor verifica la ruta en el script.")
        return
    
    # Analizar cada split
    all_stats = []
    for split in SPLITS:
        stats = analyze_split(split)
        all_stats.append(stats)
    
    # Generar recomendaci√≥n
    recommended_res = recommend_resolution(all_stats)
    
    # Generar gr√°ficas
    print(f"\n{'='*80}")
    print("üìä GENERANDO GR√ÅFICAS...")
    print(f"{'='*80}")
    plot_distributions(all_stats)
    
    print(f"\n{'='*80}")
    print("‚úÖ AN√ÅLISIS COMPLETADO")
    print(f"{'='*80}")
    print(f"\nResoluci√≥n recomendada: {recommended_res}√ó{recommended_res} px")
    print("\nConsulta las gr√°ficas en: ./analysis_plots/")


if __name__ == '__main__':
    main()