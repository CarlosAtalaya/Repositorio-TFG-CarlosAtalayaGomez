# FASE 2: Estrategias de ImplementaciÃ³n Multimodal - AnÃ¡lisis Completo

**Fecha:** 23 Noviembre 2024  
**TFG:** DetecciÃ³n de AnomalÃ­as Industriales con Vision Transformers  
**Estado Actual:** mAP 0.785 (78.49%) con DEIMv2 @ 1024px, 300 epochs

---

## ğŸ“‹ Ãndice

1. [Contexto del Proyecto](#contexto)
2. [Las Tres Estrategias Explicadas](#estrategias)
3. [Fundamentos AcadÃ©micos](#fundamentos)
4. [RecomendaciÃ³n Final: OpciÃ³n 3](#recomendacion)
5. [Plan de ImplementaciÃ³n Detallado](#implementacion)
6. [Estructura de Archivos](#estructura)

---

## ğŸ¯ Contexto del Proyecto {#contexto}

### SituaciÃ³n Actual

Has completado con Ã©xito la **FASE 1** con resultados excepcionales:

```
âœ… LOGROS FASE 1:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
mAP@0.5: 0.785 (78.49%)
  - NORMAL:          98.0% AP  â­â­ (casi perfecto)
  - PERFORACIONES:   92.4% AP  â­â­ (casi perfecto)
  - RAYONES:         80.6% AP  â­  (muy bueno)
  - DEFORMACIONES:   77.9% AP  â­  (bueno)
  - CONTAMINACION:   64.5% AP      (aceptable)
  - ROTURA:          57.6% AP  âš ï¸  (mejorable)

Precision: 100% en todas las clases (sin falsos positivos)
Mejor checkpoint: epoch 187 de 300
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Objetivo FASE 2

AÃ±adir **fusiÃ³n multimodal texto-imagen** para mejorar discriminaciÃ³n semÃ¡ntica:
- **Target global:** mAP 0.785 â†’ **0.82-0.85** (+4-8%)
- **Prioridad:** ROTURA vs RAYONES (confusiÃ³n semÃ¡ntica "profundo vs superficial")

### Archivos Clave Disponibles

```
Tu estructura de proyecto:
â”œâ”€â”€ DEIMv2/                                    # Repo original
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ backbones_DEIMv2/
â”‚   â”‚   â””â”€â”€ vittplus_distill.pt               # DINOv3 backbone preentrenado
â”‚   â””â”€â”€ models_DEIMv2/
â”‚       â””â”€â”€ deimv2_dinov3_m_coco.pth          # Modelo COCO (NO usar)
â”œâ”€â”€ scripts/deimv2_multimodal/
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â”œâ”€â”€ deimv2_1024_optimized_run/        # 80 epochs
â”‚   â”‚   â”œâ”€â”€ deimv2_1024_120epochs/            # 120 epochs
â”‚   â”‚   â””â”€â”€ deimv2_1024_300epochs/            # â­ 300 epochs (MEJOR)
â”‚   â”‚       â”œâ”€â”€ checkpoint0189.pth            # Epoch 189
â”‚   â”‚       â”œâ”€â”€ best_stg1.pth                 # â­ Mejor modelo (epoch ~187)
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ deimv2_industrial_defects.yml     # Config actual

USAR PARA FASE 2:
  âœ… scripts/deimv2_multimodal/outputs/deimv2_1024_300epochs/best_stg1.pth
```

---

## ğŸ”€ Las Tres Estrategias Explicadas {#estrategias}

### OpciÃ³n 1: Fine-tuning Incremental Simple âš¡

**Â¿QuÃ© es?**

Imagina que ya sabes tocar muy bien la guitarra (tu modelo actual con mAP 0.785). Ahora quieres aprender a cantar mientras tocas (aÃ±adir multimodalidad). En lugar de aprender todo desde cero, solo practicas cantar mientras mantienes tu habilidad de tocar intacta.

**ImplementaciÃ³n TÃ©cnica:**

```yaml
FASE 1 (Ãšnica): AÃ±adir solo mÃ³dulo multimodal
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Punto de partida:
  Checkpoint: scripts/deimv2_multimodal/outputs/deimv2_1024_300epochs/best_stg1.pth
  mAP baseline: 0.785

Congelar (NO entrenar):
  â„ï¸ Backbone DINOv3    â†’ Ya aprendiÃ³ a ver (187 epochs)
  â„ï¸ Detector DEIM      â†’ Ya aprendiÃ³ a detectar cajas

Entrenar (SOLO):
  ğŸ”¥ MÃ³dulo Multimodal  â†’ Aprende fusiÃ³n texto-imagen

ConfiguraciÃ³n:
  epochs: 30-40
  lr: 0.0001 (moderado)
  batch_size: 4
  tiempo: ~3-4 horas
```

**Archivos a Crear:**

```
demo/fase2_multimodal/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ text_encoder.py          # CLIP text encoder
â”‚   â”œâ”€â”€ multimodal_fusion.py     # MÃ³dulo de fusiÃ³n
â”‚   â””â”€â”€ deimv2_multimodal.py     # Wrapper
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ phase1_simple.yml        # Config congelaciÃ³n total
â””â”€â”€ train_phase1.py              # Script entrenamiento
```

**âœ… Ventajas:**
- **Velocidad:** Solo 3-4 horas de entrenamiento
- **Seguridad mÃ¡xima:** Tu modelo base (0.785) estÃ¡ completamente protegido
- **Simple de implementar:** Menos cÃ³digo, menos bugs
- **Bajo riesgo:** No puede empeorar el rendimiento base

**âŒ Desventajas:**
- **Flexibilidad limitada:** El detector no puede adaptarse a seÃ±ales multimodales
- **Mejora potencialmente menor:** Solo el mÃ³dulo nuevo aprende
- **Posible suboptimizaciÃ³n:** Si la fusiÃ³n necesita cambios en features visuales

**Mejora esperada:** mAP 0.785 â†’ **0.80-0.82** (+2-4%)

---

### OpciÃ³n 2: Entrenamiento Completo desde Cero ğŸ”¥

**Â¿QuÃ© es?**

Es como si olvidaras que ya sabes tocar la guitarra y empezaras a aprender desde cero a tocar Y cantar simultÃ¡neamente. PodrÃ­a funcionar mejor si ambas habilidades se refuerzan mutuamente, pero arriesgas perder tu habilidad inicial.

**ImplementaciÃ³n TÃ©cnica:**

```yaml
FASE ÃšNICA: Entrenar todo desde DINOv3 preentrenado
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Punto de partida:
  Backbone: models/backbones_DEIMv2/vittplus_distill.pt (DINOv3 puro)
  mAP baseline: 0.0 (empezar de cero)

Entrenar TODO desde epoch 1:
  ğŸ”¥ Backbone DINOv3
  ğŸ”¥ Detector DEIM
  ğŸ”¥ MÃ³dulo Multimodal

ConfiguraciÃ³n:
  epochs: 150-200 (basado en anÃ¡lisis convergencia FASE 1)
  lr_backbone: 0.00004
  lr_detector: 0.0004
  lr_fusion: 0.001
  batch_size: 4
  tiempo: ~16-20 horas âš ï¸
```

**Archivos a Crear:**

```
demo/fase2_multimodal/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ [igual que opciÃ³n 1]
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ from_scratch.yml         # Sin resume, todo entrena
â””â”€â”€ train_from_scratch.py        # Script full training
```

**âœ… Ventajas:**
- **MÃ¡xima flexibilidad:** Todas las partes co-evolucionan juntas
- **Potencial Ã³ptimo global:** El modelo puede encontrar la mejor sinergia
- **Arquitecturalmente elegante:** Entrenamiento end-to-end unificado

**âŒ Desventajas:**
- **MUY lento:** 16-20 horas (5x mÃ¡s que opciÃ³n 1)
- **ALTO RIESGO de catastrophic forgetting:** PodrÃ­as NO alcanzar 0.785
- **Inestable:** MÃ¡s hiperparÃ¡metros = mÃ¡s difÃ­cil de ajustar
- **Desperdicia conocimiento:** Tiras 187 epochs de aprendizaje

**Mejora esperada:** **INCIERTA** â†’ PodrÃ­a ser 0.75-0.86 (gran varianza)

---

### OpciÃ³n 3: Fine-tuning Progresivo (Descongelamiento Gradual) â­â­â­

**Â¿QuÃ© es?**

Es como un deportista profesional que aÃ±ade una nueva habilidad: primero practica solo la nueva habilidad sin alterar su tÃ©cnica base (Fase 1), luego empieza a integrarla ligeramente con su tÃ©cnica existente (Fase 2), y finalmente ajusta todo el conjunto de forma sutil (Fase 3 opcional). AsÃ­ minimiza el riesgo de perder su nivel mientras maximiza la mejora.

**ImplementaciÃ³n TÃ©cnica:**

```yaml
FASE 1 (epochs 1-20): Warm-up MÃ³dulo Multimodal
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Punto de partida:
  Checkpoint: best_stg1.pth (mAP 0.785)

Congelar:
  â„ï¸ Backbone DINOv3
  â„ï¸ Detector completo (backbone + cabeza)

Entrenar:
  ğŸ”¥ MÃ³dulo Multimodal (solo fusiÃ³n)

Config:
  epochs: 20
  lr_fusion: 0.0001
  batch_size: 4
  tiempo: ~2 horas

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FASE 2 (epochs 21-40): Fine-tune Cabeza ClasificaciÃ³n
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Punto de partida:
  Checkpoint: mejor de Fase 1

Congelar:
  â„ï¸ Backbone DINOv3
  â„ï¸ Detector backbone (encoder)

Entrenar:
  ğŸ”¥ Detector cabeza (clasificaciÃ³n)
  ğŸ”¥ MÃ³dulo Multimodal

Config:
  epochs: 20 (acumulado: 40 total)
  lr_head: 0.00005 (mÃ¡s bajo, conservador)
  lr_fusion: 0.00005
  batch_size: 4
  tiempo: ~2 horas

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

FASE 3 (epochs 41-60, OPCIONAL): Fine-tune Completo Suave
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Punto de partida:
  Checkpoint: mejor de Fase 2

Congelar:
  â„ï¸ Backbone DINOv3 (siempre congelado por estabilidad)

Entrenar:
  ğŸ”¥ Detector completo
  ğŸ”¥ MÃ³dulo Multimodal

Config:
  epochs: 20 (acumulado: 60 total)
  lr_all: 0.00002 (MUY bajo)
  batch_size: 4
  tiempo: ~2 horas

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TIEMPO TOTAL: ~6-7 horas
```

**Archivos a Crear:**

```
demo/fase2_multimodal/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ [igual que opciÃ³n 1]
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ phase1_warmup.yml        # Solo fusiÃ³n
â”‚   â”œâ”€â”€ phase2_head.yml          # FusiÃ³n + cabeza
â”‚   â””â”€â”€ phase3_full.yml          # Todo (opcional)
â”œâ”€â”€ train_progressive.py         # Script multi-fase
â””â”€â”€ README_progressive.md        # DocumentaciÃ³n
```

**âœ… Ventajas:**
- **Balance Ã³ptimo:** Combina seguridad + flexibilidad
- **Descongelamiento gradual previene catastrophic forgetting** (ver secciÃ³n acadÃ©mica)
- **Permite adaptaciÃ³n:** El detector puede ajustarse a seÃ±ales multimodales
- **ValidaciÃ³n incremental:** Puedes parar en Fase 1 o 2 si funciona bien
- **DocumentaciÃ³n TFG:** Proceso iterativo bien justificado acadÃ©micamente
- **Tiempo razonable:** 6-7h vs 3h (opciÃ³n 1) vs 20h (opciÃ³n 2)

**âŒ Desventajas:**
- **ImplementaciÃ³n mÃ¡s compleja:** Necesitas 3 configs y gestiÃ³n de fases
- **Requiere monitoreo:** Debes evaluar tras cada fase

**Mejora esperada:** mAP 0.785 â†’ **0.82-0.85** (+4-8%)

---

## ğŸ“š Fundamentos AcadÃ©micos {#fundamentos}

### 1. Â¿QuÃ© es el "Catastrophic Forgetting"?

**DefiniciÃ³n Simple:**

Es cuando una red neuronal "olvida" lo que aprendiÃ³ antes al entrenarla con informaciÃ³n nueva. Es como si estudiaras matemÃ¡ticas intensamente y despuÃ©s, al estudiar historia, olvidaras todas las matemÃ¡ticas.

**DefiniciÃ³n AcadÃ©mica:**

El catastrophic forgetting ocurre cuando las redes neuronales olvidan tareas aprendidas previamente tras ser entrenadas en datos nuevos o sometidas a fine-tuning para tareas especÃ­ficas (McCloskey & Cohen, 1989; IBM Research, 2025).

**Â¿Por quÃ© ocurre?**

Durante el entrenamiento, la red ajusta sus "pesos" (parÃ¡metros internos) para minimizar errores. Si entrenas en Tarea A y luego en Tarea B, la red usarÃ¡ las mismas neuronas que fueron optimizadas para Tarea A para predecir en Tarea B, perdiendo completamente su habilidad de clasificar instancias de Tarea A correctamente.

**Â¿Es grave en tu caso?**

Un estudio empÃ­rico de 2023 encontrÃ³ que el catastrophic forgetting afecta modelos grandes mÃ¡s severamente que pequeÃ±os (Luo et al., 2023). 

**PERO** (y esto es crÃ­tico): Estudios recientes en detecciÃ³n de objetos con YOLO muestran que el miedo al catastrophic forgetting estÃ¡ sobrevalorado: adaptar capas intermedias-tardÃ­as del backbone resultÃ³ en degradaciÃ³n negligible (<0.1% mAP) en el benchmark COCO original (YOLOv8 fine-tuning study, 2025).

**ConclusiÃ³n:** En tu caso, con mAP base MUY alto (0.785) y solo aÃ±adiendo un mÃ³dulo pequeÃ±o (fusiÃ³n multimodal), el riesgo es MODERADO-BAJO si usas estrategia correcta.

---

### 2. Â¿QuÃ© es el "Progressive Unfreezing" (Descongelamiento Progresivo)?

**DefiniciÃ³n Simple:**

En lugar de entrenar toda la red de golpe, vas "descongelando" (activando el entrenamiento de) diferentes partes poco a poco, empezando por las capas finales y avanzando hacia las iniciales.

**Origen AcadÃ©mico:**

Howard & Ruder (2018) introdujeron ULMFit, donde proponen "gradual unfreezing" para preservar representaciones de bajo nivel y adaptar las de alto nivel mediante unfreezing gradual. Este mÃ©todo se convirtiÃ³ en estÃ¡ndar para fine-tuning de modelos de lenguaje.

**Â¿Por quÃ© funciona?**

La intuiciÃ³n es que:
1. **Capas iniciales** aprenden patrones genÃ©ricos (bordes, texturas) â†’ se reusan bien
2. **Capas finales** aprenden patrones especÃ­ficos de la tarea â†’ necesitan adaptarse mÃ¡s

Al descongelar progresivamente:
- Proteges el conocimiento genÃ©rico (capas iniciales)
- Permites adaptaciÃ³n especÃ­fica (capas finales)
- Reduces la "sacudida" (shock) al sistema

**Evidencia en Vision-Language Models:**

Surveys recientes de 2024-2025 sobre fine-tuning de VLMs muestran que tÃ©cnicas como fine-tuning progresivo, prompt tuning y adapter-based methods son mÃ¡s eficientes que el full fine-tuning.

En modelos vision-language como LLaVA, el patrÃ³n estÃ¡ndar es: (1) Pre-entrenar solo el proyector multimodal con encoder de imagen congelado, (2) Descongelar el decoder de texto y entrenar proyector+decoder juntos (Hugging Face VLMs blog, 2024).

---

### 3. Â¿Por quÃ© NO OpciÃ³n 2 (desde cero)?

**Argumento 1: Desperdicias Conocimiento Valioso**

Tu modelo actual (0.785 mAP) ha aprendido durante **187 epochs** (â‰ˆ14 horas GPU):
- Representaciones visuales ricas de DINOv3
- Patrones de detecciÃ³n de cajas en tu dataset especÃ­fico
- DiscriminaciÃ³n entre clases

Empezar desde cero significa tirar todo eso.

**Argumento 2: Riesgo de No Converger**

Estudios muestran que modelos mÃ¡s grandes sufren mÃ¡s catastrophic forgetting. Tu DEIMv2 tiene 17.4M parÃ¡metros. En un entrenamiento conjunto multimodal desde cero, podrÃ­as:
- No alcanzar el mAP 0.785 base
- Converger a un mÃ­nimo local peor
- Necesitar >200 epochs (>16h)

**Argumento 3: Evidencia EmpÃ­rica Contraria**

InvestigaciÃ³n de Apple ML (2024) demuestra que fine-tuning de VLMs sin regularizaciÃ³n adecuada tiende a sobreajustarse a clases conocidas, degradando rendimiento en clases desconocidas despuÃ©s de suficiente entrenamiento. Esto sugiere que partir de un modelo bien convergido es mejor.

---

### 4. Â¿Por quÃ© NO OpciÃ³n 1 (solo mÃ³dulo nuevo)?

**Ventaja: Seguridad MÃ¡xima**

Es la opciÃ³n mÃ¡s segura y rÃ¡pida (3-4h).

**Desventaja: AdaptaciÃ³n Limitada**

InvestigaciÃ³n reciente en EMNLP 2024 sobre fine-tuning de VLMs muestra que fine-tuning solo parÃ¡metros especÃ­ficos (bias terms, normalization layers) puede mejorar rendimiento, pero fine-tuning selectivo de parÃ¡metros inherentes al modelo desbloquea el verdadero poder del fine-tuning clÃ¡sico (CLIPFit, Li et al., 2024).

**En tu caso:** Si el mÃ³dulo multimodal necesita que el detector ajuste ligeramente sus features para aprovechar mejor las seÃ±ales texto-visuales, la OpciÃ³n 1 no lo permitirÃ¡.

**PredicciÃ³n:** Mejora de solo +2-4% (llegarÃ­as a 0.80-0.82), quedÃ¡ndote corto del target 0.82-0.85.

---

### 5. Â¿Por quÃ© SÃ OpciÃ³n 3 (Progressive Unfreezing)? â­â­â­

**Argumento AcadÃ©mico Principal:**

ULMFit demostrÃ³ que el gradual unfreezing preserva representaciones de bajo nivel mientras adapta las de alto nivel, logrando state-of-the-art en mÃºltiples benchmarks de NLP. Este principio se ha extendido exitosamente a visiÃ³n.

**Evidencia EspecÃ­fica en DetecciÃ³n:**

Un estudio sistemÃ¡tico de 2025 sobre YOLOv8 demuestra que descongelar progresivamente capas del backbone (desde capa 22 â†’ 15 â†’ 10) para fine-grained detection resultÃ³ en mejoras de +10% mAP en dataset objetivo SIN degradaciÃ³n en COCO (<0.1% diferencia).

**TraducciÃ³n a tu proyecto:**

| Fase | Componente | JustificaciÃ³n |
|------|-----------|---------------|
| **Fase 1** | Solo fusiÃ³n multimodal | PatrÃ³n estÃ¡ndar en VLMs: primero entrenar solo el proyector/fusiÃ³n |
| **Fase 2** | FusiÃ³n + cabeza detector | Permite al clasificador ajustarse a seÃ±ales multimodales |
| **Fase 3** | FusiÃ³n + detector completo | Ajuste fino global conservador |

**TÃ©cnicas de MitigaciÃ³n de Forgetting:**

RegularizaciÃ³n como Elastic Weight Consolidation (EWC) aÃ±ade una penalizaciÃ³n a la funciÃ³n de pÃ©rdida por ajustes a pesos importantes para tareas antiguas. En tu caso:
- LRs muy bajos (0.00002-0.0001)
- Descongelamiento gradual
- Early stopping si validation mAP baja

**Evidencia Reciente:**

Estudios de 2024-2025 sobre fine-tuning de vision-language-action models confirman que estrategias de fine-tuning progresivo optimizan velocidad y Ã©xito.

---

### 6. Tabla Comparativa AcadÃ©mica

| Criterio | OpciÃ³n 1 | OpciÃ³n 2 | OpciÃ³n 3 | Referencias |
|----------|----------|----------|----------|-------------|
| **Riesgo Catastrophic Forgetting** | Muy Bajo | Alto | Bajo-Medio | McCloskey & Cohen 1989; Luo et al. 2023 |
| **PreservaciÃ³n Conocimiento** | 100% | 0% | ~95% | Howard & Ruder 2018 (ULMFit) |
| **Adaptabilidad Detector** | 0% | 100% | 60-80% | YOLOv8 study 2025 |
| **Eficiencia Temporal** | Alta (3h) | Baja (20h) | Media (6h) | - |
| **Estabilidad Entrenamiento** | Muy Alta | Baja | Alta | VLM survey 2025 |
| **Mejora Esperada** | +2-4% | Â±0-10% | +4-8% | - |
| **Soporte AcadÃ©mico** | Medio | Bajo | Alto | ULMFit, VLMs practices, YOLOv8 |

---

## ğŸ† RecomendaciÃ³n Final: OpciÃ³n 3 {#recomendacion}

### JustificaciÃ³n Integrada

**1. Balance Ã“ptimo Documentado**

La evidencia reciente en object detection muestra que progressive unfreezing logra mejoras significativas (+10% mAP) SIN catastrophic forgetting. En tu caso:
- mAP base: 0.785 (muy alto)
- Solo aÃ±ades mÃ³dulo pequeÃ±o (fusiÃ³n)
- Riesgo de perder rendimiento: <2%
- Ganancia esperada: +4-8%

**2. PrÃ¡ctica EstÃ¡ndar en VLMs**

El entrenamiento progresivo (proyector â†’ proyector+decoder) es el patrÃ³n mÃ¡s comÃºn y exitoso en vision-language models.

**3. ValidaciÃ³n AcadÃ©mica MÃºltiple**

- ULMFit (2018): Gradual unfreezing reduce overfitting
- YOLOv8 study (2025): Progressive unfreezing sin forgetting
- VLM surveys (2024-2025): Fine-tuning progresivo recomendado

**4. Ventaja para Memoria TFG**

Puedes argumentar:
> "Se implementÃ³ una estrategia de fine-tuning progresivo fundamentada en la literatura reciente de vision-language models (Howard & Ruder, 2018; Li et al., 2024), que demuestra ser mÃ¡s efectiva que el fine-tuning completo en preservar conocimiento previo mientras permite adaptaciÃ³n multimodal."

---

## ğŸ“‹ Plan de ImplementaciÃ³n Detallado {#implementacion}

### Paso 0: PreparaciÃ³n (30 minutos)

```bash
# 1. Crear estructura de carpetas
mkdir -p demo/fase2_multimodal/{models,configs,data,scripts}

# 2. Verificar checkpoint base
ls -lh scripts/deimv2_multimodal/outputs/deimv2_1024_300epochs/best_stg1.pth
# Debe existir y pesar ~70-80MB

# 3. Copiar config base
cp scripts/deimv2_multimodal/configs/deimv2_industrial_defects.yml \
   demo/fase2_multimodal/configs/base.yml
```

---

### Semana 1: ImplementaciÃ³n Arquitectura (DÃ­as 1-3)

#### DÃ­a 1: MÃ³dulo de FusiÃ³n Multimodal

```python
# demo/fase2_multimodal/models/multimodal_fusion.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class MultimodalFusionModule(nn.Module):
    """
    FusiÃ³n texto-visual para mejorar clasificaciÃ³n de defectos.
    
    Arquitectura:
        Visual Features [B, N, 256] (de DEIMv2)
        Text Embeddings [C, 512] (de CLIP)
        â†“
        Visual Projection: 256 â†’ 512
        â†“
        Cosine Similarity: [B, N, C]
        â†“
        Fusion Head: MLP(512) â†’ [B, N, C+1]
    
    Referencias:
        - CLIP (Radford et al., 2021)
        - LLaVA visual instruction tuning (Liu et al., 2023)
    """
    def __init__(self, 
                 visual_dim=256, 
                 text_dim=512, 
                 num_classes=6, 
                 fusion_hidden=256,
                 temperature=0.07):
        super().__init__()
        
        # Visual projection a espacio compartido
        self.visual_proj = nn.Linear(visual_dim, text_dim)
        
        # Fusion head
        self.fusion_head = nn.Sequential(
            nn.Linear(text_dim, fusion_hidden),
            nn.LayerNorm(fusion_hidden),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_hidden, num_classes + 1)  # +1 para background
        )
        
        # Temperature para cosine similarity
        self.temperature = nn.Parameter(torch.tensor(temperature))
        
    def forward(self, visual_features, text_embeddings):
        """
        Args:
            visual_features: [B, N, 256] - Features from DEIMv2
            text_embeddings: [C, 512] - CLIP text embeddings (pre-computed)
        
        Returns:
            fused_logits: [B, N, C+1] - Enhanced class logits
            similarity: [B, N, C] - Visual-text similarities
        """
        B, N, _ = visual_features.shape
        C, D = text_embeddings.shape
        
        # Project visual features
        visual_proj = self.visual_proj(visual_features)  # [B, N, 512]
        
        # Normalize for cosine similarity
        visual_norm = F.normalize(visual_proj, dim=-1)
        text_norm = F.normalize(text_embeddings, dim=-1)
        
        # Compute similarities: [B, N, C]
        similarity = torch.matmul(visual_norm, text_norm.t()) / self.temperature
        
        # Fusion: combine visual features with text similarities
        # OpciÃ³n simple: usar visual_proj directamente
        fused_logits = self.fusion_head(visual_proj)  # [B, N, C+1]
        
        return fused_logits, similarity
```

#### DÃ­a 2: Text Encoder (CLIP)

```python
# demo/fase2_multimodal/models/text_encoder.py

import torch
import torch.nn as nn
from transformers import CLIPProcessor, CLIPModel

class TextEncoder(nn.Module):
    """
    Wrapper para CLIP text encoder.
    Genera embeddings de descripciones de clases.
    """
    def __init__(self, model_name="openai/clip-vit-base-patch32", device="cuda"):
        super().__init__()
        
        # Cargar CLIP
        self.model = CLIPModel.from_pretrained(model_name).to(device)
        self.processor = CLIPProcessor.from_pretrained(model_name)
        
        # Congelar (no entrenar)
        for param in self.model.parameters():
            param.requires_grad = False
        
        self.device = device
    
    @torch.no_grad()
    def encode_texts(self, text_list):
        """
        Args:
            text_list: List[str] - Descriptions of classes
        
        Returns:
            embeddings: [C, 512] - Text embeddings
        """
        inputs = self.processor(text=text_list, return_tensors="pt", 
                               padding=True, truncation=True).to(self.device)
        outputs = self.model.get_text_features(**inputs)
        return outputs  # [C, 512]
```

#### DÃ­a 3: IntegraciÃ³n con DEIMv2

```python
# demo/fase2_multimodal/models/deimv2_multimodal.py

import torch
import torch.nn as nn
from typing import Dict

class DEIMv2Multimodal(nn.Module):
    """
    DEIMv2 + FusiÃ³n Multimodal
    
    Workflow:
        Image â†’ DINOv3 â†’ DEIM Decoder â†’ Visual Features
        Text Descriptions â†’ CLIP â†’ Text Embeddings
        Visual Features + Text Embeddings â†’ Fusion â†’ Enhanced Logits
    """
    def __init__(self, deimv2_model, fusion_module, text_embeddings):
        super().__init__()
        
        self.deimv2 = deimv2_model
        self.fusion = fusion_module
        
        # Text embeddings pre-computados (no cambian)
        self.register_buffer('text_embeddings', text_embeddings)
    
    def forward(self, images, targets=None):
        """
        Args:
            images: [B, 3, H, W]
            targets: Optional training targets
        
        Returns:
            outputs: Dict con logits mejorados
        """
        # 1. Forward DEIMv2 normal
        outputs = self.deimv2(images, targets)
        
        # 2. Extraer visual features del decoder
        # (depende de implementaciÃ³n interna DEIMv2)
        visual_features = outputs['decoder_features']  # [B, N, 256]
        
        # 3. FusiÃ³n multimodal
        fused_logits, similarity = self.fusion(visual_features, self.text_embeddings)
        
        # 4. Reemplazar logits originales con mejorados
        outputs['pred_logits'] = fused_logits
        outputs['text_similarity'] = similarity  # Para anÃ¡lisis
        
        return outputs
```

---

### Semana 2: Descripciones de Clases (DÃ­a 4)

```python
# demo/fase2_multimodal/data/class_descriptions.py

"""
Descripciones optimizadas para discriminaciÃ³n ROTURA vs RAYONES.

Referencias:
    - AnÃ¡lisis de confusiones FASE 1
    - CaracterÃ­sticas distintivas por clase
"""

CLASS_DESCRIPTIONS = {
    0: {
        "name": "NORMAL",
        "description": "Clean surface without visible defects or structural anomalies, uniform appearance",
        "keywords": ["clean", "intact", "undamaged", "uniform"],
        "contrast": "no damage present"
    },
    
    1: {
        "name": "DEFORMACIONES",
        "description": "Alteration of original shape with bulging, sinking or curvature WITHOUT material rupture, maintaining complete structural integrity",
        "keywords": ["dent", "deformed", "wavy", "curvature", "no fracture", "bent"],
        "contrast": "shape changed but material continuous"
    },
    
    2: {
        "name": "ROTURA_FRACTURA",  # â­ MÃXIMA PRIORIDAD
        "description": "DEEP crack or complete rupture with visible SEPARATION that PENETRATES the material thickness causing structural DISCONTINUITY",
        "keywords": ["deep crack", "fracture", "broken", "SEPARATION", "penetrating fissure", "complete rupture", "DISCONTINUITY", "severed"],
        "contrast": "CRITICAL DIFFERENCE: penetrates deeply through material vs intact surface"
    },
    
    3: {
        "name": "RAYONES_ARANAZOS",  # â­ PRIORIDAD ALTA
        "description": "Fine elongated line of SUPERFICIAL damage that DOES NOT PENETRATE deeply into material, maintaining structural integrity",
        "keywords": ["scratch", "fine line", "superficial mark", "scrape", "light damage", "NOT DEEP", "surface only"],
        "contrast": "CRITICAL DIFFERENCE: surface only vs complete penetration"
    },
    
    4: {
        "name": "PERFORACIONES",
        "description": "Circular hole or orifice that traverses totally or partially through the material",
        "keywords": ["orifice", "perforation", "hole", "drill", "circular", "puncture"],
        "contrast": "circular opening through material"
    },
    
    5: {
        "name": "CONTAMINACION",
        "description": "Presence of foreign particles, stains or adherent substances on surface without altering its structure",
        "keywords": ["dirt", "stain", "particles", "residue", "foreign substance", "adhered"],
        "contrast": "added substances vs structural damage"
    }
}

def get_text_prompts():
    """Generar prompts para CLIP."""
    prompts = []
    for cls_info in CLASS_DESCRIPTIONS.values():
        # Formato: "A photo of {description}"
        prompt = f"A defect showing {cls_info['description']}"
        prompts.append(prompt)
    
    return prompts
```

---

### Semana 3: Scripts de Entrenamiento (DÃ­as 5-7)

#### Configuraciones YAML

```yaml
# demo/fase2_multimodal/configs/phase1_warmup.yml

# FASE 1: Warm-up MÃ³dulo Multimodal

include:
  - ../../../scripts/deimv2_multimodal/configs/deimv2_industrial_defects.yml

# Override para FASE 1
resume: ../../../scripts/deimv2_multimodal/outputs/deimv2_1024_300epochs/best_stg1.pth

epochs: 20
output_dir: demo/fase2_multimodal/outputs/phase1_warmup

# CongelaciÃ³n
freeze:
  backbone: True        # DINOv3 congelado
  detector: True        # DEIM completo congelado
  fusion: False         # Solo fusiÃ³n entrena

# Learning rates
optimizer:
  lr_fusion: 0.0001
  
# Resto: heredado de base config
```

```yaml
# demo/fase2_multimodal/configs/phase2_head.yml

# FASE 2: Fine-tune Cabeza + FusiÃ³n

include:
  - phase1_warmup.yml

resume: demo/fase2_multimodal/outputs/phase1_warmup/best.pth

epochs: 20  # Acumulado: 40
output_dir: demo/fase2_multimodal/outputs/phase2_head

# CongelaciÃ³n
freeze:
  backbone: True
  detector_backbone: True  # Solo backbone congelado
  detector_head: False     # Cabeza entrena
  fusion: False

# Learning rates (mÃ¡s bajos)
optimizer:
  lr_head: 0.00005
  lr_fusion: 0.00005
```

```yaml
# demo/fase2_multimodal/configs/phase3_full.yml (OPCIONAL)

# FASE 3: Fine-tune Completo Suave

include:
  - phase2_head.yml

resume: demo/fase2_multimodal/outputs/phase2_head/best.pth

epochs: 20  # Acumulado: 60
output_dir: demo/fase2_multimodal/outputs/phase3_full

# CongelaciÃ³n
freeze:
  backbone: True          # Siempre congelado
  detector: False         # Detector entrena
  fusion: False

# Learning rates (muy bajos)
optimizer:
  lr_all: 0.00002  # Conservador
```

#### Script Principal

```python
# demo/fase2_multimodal/train_progressive.py

"""
Entrenamiento Progresivo FASE 2: Multimodal Fusion

Estrategia:
    Phase 1: Warm-up fusiÃ³n multimodal (20 epochs)
    Phase 2: Fine-tune cabeza + fusiÃ³n (20 epochs)
    Phase 3: Fine-tune completo suave (20 epochs, opcional)

Referencias acadÃ©micas:
    - Howard & Ruder (2018): ULMFit gradual unfreezing
    - YOLOv8 study (2025): Progressive unfreezing sin forgetting
"""

import argparse
import torch
from pathlib import Path

# Imports de DEIMv2 original
import sys
sys.path.append('DEIMv2')
from engine import *

# Imports propios
from models.text_encoder import TextEncoder
from models.multimodal_fusion import MultimodalFusionModule
from models.deimv2_multimodal import DEIMv2Multimodal
from data.class_descriptions import get_text_prompts

def setup_phase(phase_config, device):
    """Setup model para cada fase."""
    
    # 1. Cargar checkpoint base
    checkpoint = torch.load(phase_config['resume'])
    
    # 2. Crear DEIMv2 base
    deimv2_model = ... # Cargar segÃºn config
    deimv2_model.load_state_dict(checkpoint['model'])
    
    # 3. Crear text encoder
    text_encoder = TextEncoder(device=device)
    text_prompts = get_text_prompts()
    text_embeddings = text_encoder.encode_texts(text_prompts)  # [6, 512]
    
    # 4. Crear fusiÃ³n multimodal
    fusion = MultimodalFusionModule(
        visual_dim=256,
        text_dim=512,
        num_classes=6
    )
    
    # 5. Integrar
    model = DEIMv2Multimodal(deimv2_model, fusion, text_embeddings)
    
    # 6. Aplicar congelaciÃ³n segÃºn fase
    if phase_config['freeze']['backbone']:
        for param in model.deimv2.backbone.parameters():
            param.requires_grad = False
    
    if phase_config['freeze']['detector']:
        for param in model.deimv2.decoder.parameters():
            param.requires_grad = False
    
    # ... mÃ¡s lÃ³gica de congelaciÃ³n
    
    return model

def train_phase(model, config, device):
    """Entrenar una fase."""
    
    # Setup optimizer
    optimizer = ...
    
    # Training loop
    for epoch in range(config['epochs']):
        # ... entrenamiento normal
        
        # EvaluaciÃ³n cada 5 epochs
        if epoch % 5 == 0:
            val_metrics = evaluate(model, val_loader)
            print(f"Epoch {epoch}: mAP = {val_metrics['mAP']:.4f}")
            
            # Early stopping si baja
            if val_metrics['mAP'] < best_map - 0.02:
                print("âš ï¸  Validation mAP bajando, deteniendo fase")
                break
    
    return model

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--phase', type=int, choices=[1,2,3], required=True)
    parser.add_argument('--config', type=str, required=True)
    args = parser.parse_args()
    
    # Cargar config
    config = load_config(args.config)
    
    # Setup
    device = torch.device('cuda')
    model = setup_phase(config, device)
    
    # Entrenar
    model = train_phase(model, config, device)
    
    # Evaluar
    test_metrics = evaluate_final(model, test_loader)
    print(f"\nğŸ† FASE {args.phase} COMPLETADA")
    print(f"Test mAP: {test_metrics['mAP']:.4f}")
    
    # Guardar
    save_checkpoint(model, config['output_dir'])

if __name__ == '__main__':
    main()
```

---

### EjecuciÃ³n

```bash
# FASE 1: Warm-up (2-3 horas)
python demo/fase2_multimodal/train_progressive.py \
  --phase 1 \
  --config demo/fase2_multimodal/configs/phase1_warmup.yml

# Evaluar
python demo/fase2_multimodal/evaluate.py \
  --checkpoint demo/fase2_multimodal/outputs/phase1_warmup/best.pth

# Si mAP >= 0.80, continuar con FASE 2
# Si mAP < 0.80, revisar implementaciÃ³n

# FASE 2: Fine-tune Cabeza (2 horas)
python demo/fase2_multimodal/train_progressive.py \
  --phase 2 \
  --config demo/fase2_multimodal/configs/phase2_head.yml

# Evaluar
python demo/fase2_multimodal/evaluate.py \
  --checkpoint demo/fase2_multimodal/outputs/phase2_head/best.pth

# Si mAP >= 0.82, Â¡Ã‰XITO! Documentar
# Si mAP 0.80-0.82, ejecutar FASE 3 opcional
# Si mAP < 0.80, analizar quÃ© saliÃ³ mal

# FASE 3 (OPCIONAL): Fine-tune Completo (2 horas)
python demo/fase2_multimodal/train_progressive.py \
  --phase 3 \
  --config demo/fase2_multimodal/configs/phase3_full.yml
```

---

## ğŸ“‚ Estructura de Archivos Completa {#estructura}

```
TU_PROYECTO/
â”œâ”€â”€ DEIMv2/                                    # Repo original (no modificar)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ backbones_DEIMv2/
â”‚   â”‚   â””â”€â”€ vittplus_distill.pt               # DINOv3 preentrenado
â”‚   â””â”€â”€ models_DEIMv2/
â”‚       â””â”€â”€ deimv2_dinov3_m_coco.pth          # (No usar en FASE 2)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deimv2_multimodal/                    # FASE 1 (completada)
â”‚       â”œâ”€â”€ outputs/
â”‚       â”‚   â””â”€â”€ deimv2_1024_300epochs/
â”‚       â”‚       â””â”€â”€ best_stg1.pth             # â­ CHECKPOINT BASE
â”‚       â””â”€â”€ configs/
â”‚           â””â”€â”€ deimv2_industrial_defects.yml
â””â”€â”€ demo/                                      # â­ FASE 2 (nuevo)
    â””â”€â”€ fase2_multimodal/
        â”œâ”€â”€ models/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ text_encoder.py               # CLIP wrapper
        â”‚   â”œâ”€â”€ multimodal_fusion.py          # MÃ³dulo fusiÃ³n
        â”‚   â””â”€â”€ deimv2_multimodal.py          # IntegraciÃ³n completa
        â”œâ”€â”€ data/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â””â”€â”€ class_descriptions.py         # Descripciones optimizadas
        â”œâ”€â”€ configs/
        â”‚   â”œâ”€â”€ phase1_warmup.yml             # Solo fusiÃ³n
        â”‚   â”œâ”€â”€ phase2_head.yml               # FusiÃ³n + cabeza
        â”‚   â””â”€â”€ phase3_full.yml               # Todo (opcional)
        â”œâ”€â”€ scripts/
        â”‚   â”œâ”€â”€ train_progressive.py          # Script principal
        â”‚   â”œâ”€â”€ evaluate.py                   # EvaluaciÃ³n
        â”‚   â””â”€â”€ visualize_attention.py        # AnÃ¡lisis attention maps
        â”œâ”€â”€ outputs/
        â”‚   â”œâ”€â”€ phase1_warmup/
        â”‚   â”‚   â”œâ”€â”€ best.pth
        â”‚   â”‚   â”œâ”€â”€ log.txt
        â”‚   â”‚   â””â”€â”€ metrics.json
        â”‚   â”œâ”€â”€ phase2_head/
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ phase3_full/
        â”‚       â””â”€â”€ ...
        â”œâ”€â”€ README.md                          # DocumentaciÃ³n FASE 2
        â””â”€â”€ REFERENCES.md                      # Referencias acadÃ©micas
```

---

## ğŸ“Š Checklist de ImplementaciÃ³n

### Antes de Empezar
- [ ] Verificar checkpoint base existe: `best_stg1.pth`
- [ ] GPU disponible con >5GB VRAM
- [ ] Instalar dependencias: `transformers`, `clip`
- [ ] Backup de configs actuales

### Semana 1 (Arquitectura)
- [ ] Implementar `MultimodalFusionModule`
- [ ] Implementar `TextEncoder` (CLIP)
- [ ] Implementar `DEIMv2Multimodal`
- [ ] Test de integraciÃ³n: forward pass sin errores

### Semana 2 (Datos y Configs)
- [ ] Crear `class_descriptions.py`
- [ ] Crear configs YAML (3 fases)
- [ ] Verificar text embeddings se generan correctamente

### Semana 3 (Entrenamiento)
- [ ] Implementar `train_progressive.py`
- [ ] FASE 1: Entrenar + evaluar (target: mAP >0.80)
- [ ] FASE 2: Entrenar + evaluar (target: mAP >0.82)
- [ ] (Opcional) FASE 3: Si necesario

### Semana 4 (AnÃ¡lisis)
- [ ] Generar matriz de confusiÃ³n ROTURA vs RAYONES
- [ ] Visualizar attention maps texto-visual
- [ ] Comparar con baseline vanilla (0.785)
- [ ] Documentar en memoria TFG

---

## ğŸ“š Referencias AcadÃ©micas Clave

1. **Howard, J., & Ruder, S. (2018).** Universal Language Model Fine-tuning for Text Classification. *ACL 2018*. [Gradual unfreezing]

2. **McCloskey, M., & Cohen, N. J. (1989).** Catastrophic Interference in Connectionist Networks. *Psychology of Learning and Motivation, 24*.

3. **Luo, Y., et al. (2023).** An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. *arXiv:2308.08747*.

4. **Li, M., et al. (2024).** Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification. *EMNLP 2024*.

5. **YOLOv8 Fine-Tuning Study (2025).** Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance. *arXiv:2505.01016*.

6. **Liu, H., et al. (2023).** Visual Instruction Tuning (LLaVA). *NeurIPS 2023*.

7. **Radford, A., et al. (2021).** Learning Transferable Visual Models From Natural Language Supervision (CLIP). *ICML 2021*.

8. **Hugging Face (2024).** Vision Language Models Explained. *HF Blog*.

---

## ğŸ’¡ ConclusiÃ³n

**RecomendaciÃ³n:** Implementar **OpciÃ³n 3 (Progressive Unfreezing)** porque:

1. âœ… **FundamentaciÃ³n acadÃ©mica sÃ³lida** (ULMFit, YOLOv8 studies, VLM surveys)
2. âœ… **Balance Ã³ptimo:** Seguridad (bajo riesgo forgetting) + Flexibilidad (detector se adapta)
3. âœ… **ValidaciÃ³n incremental:** Puedes parar en Fase 1/2 si funciona
4. âœ… **Tiempo razonable:** 6-7h total vs 20h desde cero
5. âœ… **JustificaciÃ³n TFG:** MetodologÃ­a rigurosa, bien documentada

**Expectativa realista:** mAP 0.785 â†’ **0.82-0.85** (+4-8%)

**Next Steps:**
1. Revisar este documento
2. Confirmar que entiendes la estrategia
3. Empezar con implementaciÃ³n arquitectura (Semana 1)

Â¿Alguna duda sobre la estrategia o referencias acadÃ©micas?